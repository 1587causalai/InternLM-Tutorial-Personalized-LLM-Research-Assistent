# Jason Wei 谈大语言模型评估

本文总结了Jason Wei在其[博客](https://www.jasonwei.net/blog/evals)中关于大语言模型（LLMs）评估的见解，以及由此引发的一些关键问题和疑惑。

## 关键问题及其答案

1. **为什么评估基准在研究社区中如此重要？**
   - 评估基准是研究社区的激励因素，与突破性成果密切相关。突破性成果通常与评估基准上的显著性能提升有关。

2. **一个成功的评估基准的标准是什么？**
   - 成功的评估基准被突破性论文使用，并且在研究社区内被信任。

3. **过去五年中有哪些成功的评估基准？**
   - 成功的评估基准包括GLUE/SuperGLUE、MMLU、GSM8K、MATH和HumanEval，以及其他如HellaSwag和SQuAD。

4. **成功的评估基准有哪些共同特征？**
   - 成功的评估基准通常伴随着重要论文的推广和胜利。例如，GLUE与BERT，MMLU与Gopher、Chinchilla、Flan-PaLM，GSM8K与思维链突破，MATH与Minerva，HumanEval与Codex等。


## 深入分析及见解

1. **评估基准的命名问题**
   - 很多优秀的评估基准有糟糕的名字，例如GSM8K不需要“8K”，HumanEval实际上不使用人类进行评估（因为问题是由人类创建的），MATH太通用，因此被称为“Hendrycks-math”。

2. **如何推广评估基准**
   - 帮助人们使用你的评估基准。例如，为别人运行评估基准，如果他们的模型表现良好，他们会喜欢并推广它。
   - 创建激励机制使人们使用你的评估基准。例如，得到实验室或公司的管理者的支持，要求他们的团队使用它。

3. **常见的不成功评估基准的错误**
   - **样本量不足**：如果评估基准没有足够的样本，结果会很噪声，并且用户界面体验不佳。例如，GPQA尽管是个不错的评估基准，但由于其结果会随提示词波动，使其难以使用。建议评估基准至少有1000个样本，尤其是多选题评估。
   - **质量不高**：评估基准中如果有很多错误，研究者会失去信任。例如，Natural Questions（NQ），因为GPT-4的表现超越了评估基准提供的真实答案，所以被放弃使用。
   - **过于复杂**：如果评估基准过于复杂，研究者会难以理解和使用。例如，HELM的第一个版本虽然是个很好的尝试，但包含了太多指标和子集，导致使用率低。好的评估基准通常有一个单一的数值指标。
   - **运行耗时过长**：即使评估基准的其他方面都很好，但如果运行耗时过长，也难以获得广泛使用。例如，BIG-Bench虽然信号强大，但由于运行复杂和耗时而没有获得广泛应用。
   - **任务不具备意义**：如果评估基准的任务不具备意义，研究者不会关注。例如，BIG-Bench Hard中的某些任务尽管具有挑战性，但其结果无法得出关于模型智能的有意义结论。成功的评估基准通常衡量与智能核心相关的任务，如语言理解、考试问题或数学。
   - **评分必须准确**：如果评估基准的评分不准确，研究者在调试模型时发现评分错误，会立即放弃使用该评估基准。应尽可能减少解析错误，并使用最好的自动评分提示。
   - **长期适用性**：评估基准的性能不应过快饱和。例如，GLUE/SuperGLUE的性能饱和太快，导致难以显示大的提升，最终被弃用。语言模型在摘要和翻译任务上的进步速度比开发好的评估基准更快，因此这些任务也停止了测量。

4. **LLMs使评估变得更加困难**
   - 大语言模型（LLM）是多任务且长响应的，现有评估基准使用简单评分（如多选题、检查数字或运行单元测试），但这并不完全适用。应考虑统一使用某种提示词（如零样本思维链），尽管不完美，但能让大家统一标准。
   - 人类成对评分（如LMSYS）是一种新趋势。这类评估的优势在于提供单一数值指标，但劣势在于不清楚具体衡量的是什么。

5. **模型生成的评估**
   - 尽管模型生成的评估可能不稳定，但它们适合快速实验和观察大幅度性能提升。然而，要创建一个能经受时间考验的优秀评估基准，需要仔细考量，不能依赖合成评估。

6. **评估基准的主题**
   - 评估基准的主题决定了其受关注的程度。高质量的领域特定评估基准（如法律、医学等）需要针对领域专家的价值进行定制。例如，Jason Wei创建的组织病理学图像基准在医学图像分析领域外并未获得关注，仅有40次引用。

7. **测试集污染问题**
   - 测试集污染是评估基准的一个日益重要的问题。解决方案包括保持测试集隐藏，但这会增加摩擦。Chris Manning建议评估基准同时有公开测试集和私有测试集，并监控模型在这两者之间的表现差异。

8. **评估基准反映身份认同**
   - 你关注的评估基准反映了你的身份。PhD会对模型在数学、编码和物理方面的推理能力感兴趣；而来自软件或产品背景的工程师会认为用户评估（如LMSYS）是黄金标准。Jason Wei个人倾向于智能评估，因为他认为智能是AI与人类互动的根本驱动因素。

9. **社区应增加对评估基准的投资**
   - 尽管评估基准的开发过程痛苦且通常不如建模工作受奖励，但它们是AI研究者的目标函数，是对该领域产生影响的有力方式。

## LMSYS 评估

1. **什么是LMSYS评估？**
   - LMSYS 评估是一种通过人类成对评分（pairwise ratings）来评估模型的方法。评审者对两个模型的输出进行比较，选择更好的响应。

2. **LMSYS评估的优缺点是什么？**
   - **优点**：提供一个单一的数值指标，可以有效评估复杂任务下的模型表现；通过大量样本平均，减少单一评分中的噪音。
   - **缺点**：难以明确具体衡量的标准，可能受到评审者个人偏好的影响。

3. **LMSYS评估的应用**
   - 在许多近期研究中用于评估各种语言模型（LLMs）的表现，例如，评估不同版本的GPT模型或比较GPT与其他语言模型的表现。

## 结论

Jason Wei强调了评估基准在AI研究中的重要性，并提供了关于如何创建和推广成功评估基准的见解。社区应加大对评估基准的投资，以推动AI领域的发展。
